{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDB Data set\n",
    "Only keep the top 10,000 most frequently occurring words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/py36-venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "# Load the data as lists of integers.\n",
    "max_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the distribution of review lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max, min, mean: (2494, 11, 238.71364)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lens = list(map(len, x_train))\n",
    "print('max, min, mean: {}'.format((np.max(lens), np.min(lens), np.mean(lens))))\n",
    "plt.hist(lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad or truncate each review to make the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,  194, 1153,  194, 8255,   78,  228,    5,\n",
       "          6, 1463, 4369, 5012,  134,   26,    4,  715,    8,  118, 1634,\n",
       "         14,  394,   20,   13,  119,  954,  189,  102,    5,  207,  110,\n",
       "       3103,   21,   14,   69,  188,    8,   30,   23,    7,    4,  249,\n",
       "        126,   93,    4,  114,    9, 2300, 1523,    5,  647,    4,  116,\n",
       "          9,   35, 8163,    4,  229,    9,  340, 1322,    4,  118,    9,\n",
       "          4,  130, 4901,   19,    4, 1002,    5,   89,   29,  952,   46,\n",
       "         37,    4,  455,    9,   45,   43,   38, 1543, 1905,  398,    4,\n",
       "       1649,   26, 6853,    5,  163,   11, 3215,    2,    4, 1153,    9,\n",
       "        194,  775,    7, 8255,    2,  349, 2637,  148,  605,    2, 8003,\n",
       "         15,  123,  125,   68,    2, 6853,   15,  349,  165, 4362,   98,\n",
       "          5,    4,  228,    9,   43,    2, 1157,   15,  299,  120,    5,\n",
       "        120,  174,   11,  220,  175,  136,   50,    9, 4373,  228, 8255,\n",
       "          5,    2,  656,  245, 2350,    5,    4, 9837,  131,  152,  491,\n",
       "         18,    2,   32, 7464, 1212,   14,    9,    6,  371,   78,   22,\n",
       "        625,   64, 1382,    9,    8,  168,  145,   23,    4, 1690,   15,\n",
       "         16,    4, 1355,    5,   28,    6,   52,  154,  462,   33,   89,\n",
       "         78,  285,   16,  145,   95], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_review_len = 500\n",
    "x_train = pad_sequences(x_train, maxlen=max_review_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_review_len)\n",
    "\n",
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization: Word Embeddings\n",
    "\n",
    "Embedding Matrix is [10000, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, feature_dim, input_length=max_review_len))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.7)),\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "15000/15000 [==============================] - 3s 226us/step - loss: 0.5490 - acc: 0.6929 - val_loss: 0.3271 - val_acc: 0.8594\n",
      "Epoch 2/2\n",
      "15000/15000 [==============================] - 3s 190us/step - loss: 0.2571 - acc: 0.9013 - val_loss: 0.2870 - val_acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2927669722890854, 0.87752]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Probabilities:\n",
      " [[0.05055669]\n",
      " [0.9987041 ]\n",
      " [0.72994906]\n",
      " [0.31417215]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(x_test[:4])\n",
    "probs = model.predict_proba(x_test[:4])\n",
    "\n",
    "print(\"Predicted Classes: {}\".format(preds))\n",
    "print(\"Predicted Probabilities:\\n {}\".format(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embedding Layer's Shape: (10000, 20)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00040193,  0.00052042, -0.00347654, -0.00111846,  0.00516806,\n",
       "         0.00054633, -0.00222922,  0.00071461,  0.0035489 , -0.00062467,\n",
       "        -0.00368488,  0.00255503, -0.00199691,  0.00034472, -0.00360599,\n",
       "         0.0020104 ,  0.00608825,  0.00419473, -0.00108733, -0.00360193],\n",
       "       [-0.00930036, -0.00112146,  0.0585938 ,  0.02294737, -0.03802411,\n",
       "         0.00203338,  0.04374479, -0.02325076,  0.00796256,  0.02651064,\n",
       "         0.06300712, -0.02150084, -0.04345286, -0.01683675,  0.06180406,\n",
       "        -0.00233442, -0.03593991, -0.07857389, -0.00209842, -0.06425215],\n",
       "       [ 0.04249636, -0.00852364,  0.00597049,  0.01763395, -0.02088473,\n",
       "         0.02338243, -0.02212349, -0.00328648,  0.00600215, -0.01482508,\n",
       "        -0.03491296, -0.04678832, -0.02405351, -0.00277475, -0.00914833,\n",
       "         0.00908425,  0.04600341,  0.03514171,  0.0234772 , -0.05848712]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_weights() # get model weights\n",
    "embeddings = weights[0]\n",
    "print(\"The Embedding Layer's Shape: {}\\n\".format(embeddings.shape))\n",
    "embeddings[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "#! unzip glove.6B.zip\n",
    "#! cp glove.6B.100d.txt ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe weight file to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "glove_dir = './models/'\n",
    "embeddings_dict = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Embedding Matrix for GloVe weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "feature_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, feature_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if i < max_words and (embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1600032   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,600,065\n",
      "Trainable params: 2,600,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, feature_dim, input_length=max_review_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 237us/step - loss: 0.7420 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4948\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.7011 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4948\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.7018 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5053\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 3s 222us/step - loss: 0.6959 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.4946\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6952 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.4957\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6946 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4946\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6935 - acc: 0.5036 - val_loss: 0.6931 - val_acc: 0.4961\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6952 - acc: 0.5010 - val_loss: 0.6982 - val_acc: 0.4941\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6934 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.4949\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6949 - acc: 0.5053 - val_loss: 0.6930 - val_acc: 0.5062\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.7003 - acc: 0.5086 - val_loss: 0.6935 - val_acc: 0.5010\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6973 - acc: 0.5113 - val_loss: 0.6926 - val_acc: 0.5084\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6899 - acc: 0.5186 - val_loss: 0.6921 - val_acc: 0.5135\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6888 - acc: 0.5184 - val_loss: 0.6926 - val_acc: 0.5076\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6862 - acc: 0.5262 - val_loss: 0.6928 - val_acc: 0.5116\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 3s 222us/step - loss: 0.6852 - acc: 0.5314 - val_loss: 0.6946 - val_acc: 0.5209\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6830 - acc: 0.5328 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6828 - acc: 0.5330 - val_loss: 0.6928 - val_acc: 0.5138\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 223us/step - loss: 0.6740 - acc: 0.5379 - val_loss: 0.6923 - val_acc: 0.5214\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 3s 224us/step - loss: 0.6695 - acc: 0.5400 - val_loss: 0.6981 - val_acc: 0.5207\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "model.save_weights('./models/pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
